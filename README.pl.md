ğŸ•· Serverless Web Crawler (AWS Lambda + DynamoDB + SQS)
Internetowy crawler oparty na usÅ‚ugach AWS, napisany w jÄ™zyku Python. UmoÅ¼liwia analizowanie i indeksowanie stron internetowych, zaczynajÄ…c od podanego adresu pierwotnego root URL, nastÄ™pnie rekurencyjnie odwiedza wszystkie powiÄ…zane strony wewnÄ™trzne i wyciÄ…ga linki do tabeli DynamoDB. 

ğŸ“¦ Wymagania

    - AWS CLI i skonfigurowane uprawnienia
    - SAM CLI
    - Python 3.10+


ğŸ“ Struktura projektu 
.
â”œâ”€â”€ crawler.py                    # Druga funkcja Lambda â€“ analizuje i przetwarza strony
â”œâ”€â”€ funkcje_pomocnicze/
â”‚   â””â”€â”€ pomocnicze.py            # Funkcje wspierajÄ…ce (np. filtrowanie, logika pomocnicza)
â”œâ”€â”€ initializer.py               # Pierwsza funkcja Lambda â€“ inicjuje crawl i uruchamia caÅ‚y proces
â”œâ”€â”€ models/
â”‚   â””â”€â”€ visitedURLS.py           # Model danych zapisywany do DynamoDB
â”œâ”€â”€ requirements.txt             # Wymagane biblioteki Pythona (m.in. bs4)
â”œâ”€â”€ samconfig.toml               # Konfiguracja deploymentu AWS SAM
â”œâ”€â”€ template.yaml                # Szablon SAM definiujÄ…cy infrastrukturÄ™ jako kod
â””â”€â”€ .gitignore                   # Ignorowane pliki i katalogi (np. .aws-sam/)

ğŸš€ UÅ¼yte technologie 

AWS Lambda â€“ dwie funkcje:
    - InitializerFunction â€“ inicjuje crawl, wrzucajÄ…c linki do kolejki,
    - CrawlerFunction â€“ przetwarza linki z kolejki i zapisuje dane.

AWS SQS â€“ kolejka przechowujÄ…ca oczekujÄ…ce linki do przetworzenia.

AWS DynamoDB â€“ baza danych przechowujÄ…ca odwiedzone adresy oraz identyfikatory crawlÃ³w.

AWS SAM (Serverless Application Model) â€“ narzÄ™dzie do definiowania infrastruktury jako kodu (IaC), bÄ™dÄ…ce wyÅ¼szÄ… warstwÄ… abstrakcji nad AWS CloudFormation.

Python 3.x â€“ jÄ™zyk uÅ¼ywany do implementacji funkcji Lambda.

BeautifulSoup (bs4) â€“ biblioteka do analizy i parsowania HTML.

âš™ï¸ Jak dziaÅ‚a aplikacja?
ğŸ§  OgÃ³lny przepÅ‚yw
UÅ¼ytkownik (lub inne API) podaje root URL.
Funkcja Lambda nr 1 (initializer):
Generuje runID zÅ‚oÅ¼ony z daty i unikalnego identyfikatora (UUID).
Zapisuje root URL do bazy DynamoDB jako odwiedzony (optymalizacja).
WysyÅ‚a wiadomoÅ›Ä‡ z root URL do kolejki SQS.
Funkcja Lambda nr 2 (crawler):
Pobiera wiadomoÅ›Ä‡ z kolejki SQS.
ÅÄ…czy siÄ™ z podanÄ… stronÄ… przez HTTP.
Analizuje HTML i wyciÄ…ga linki (wewnÄ™trzne).
Filtruje tylko linki z tego samego hosta co root URL (pomija np. #, inne domeny).
Sprawdza, ktÃ³re linki sÄ… nowe (niewidoczne jeszcze w DynamoDB).
Nowe linki zapisuje do DynamoDB i dodaje ponownie do SQS.
Proces powtarza siÄ™ rekurencyjnie, aÅ¼ nie znajdzie nowych linkÃ³w.

ğŸ” SzczegÃ³Å‚y dziaÅ‚ania
ğŸ“¦ Funkcja inicjalizujÄ…ca (initializer.py)
Przypisuje kaÅ¼demu crawlâ€™owi unikatowy runID oparty o datÄ™ i UUID.

UUID to unikatowy identyfikator, ktÃ³ry pozwala uruchomiÄ‡ ten sam root URL wiele razy, bez bÅ‚Ä™du zapisu w bazie (UUID uÅ¼ywany jako sort key w tabeli).

Mimo Å¼e root URL nie zostaÅ‚ jeszcze odwiedzony, zaznaczamy go jako visited, by zapobiec cyklom i duplikatom â€“ to celowa technika optymalizacyjna.

PrzesyÅ‚a wiadomoÅ›Ä‡ do SQS z root URL.

ğŸ” Funkcja crawlera (crawler.py)
DziaÅ‚a rekurencyjnie:

ğŸŒ€ Pierwsza iteracja:
Pobiera root URL z SQS (jedyny link do tej pory).

ÅÄ…czy siÄ™ z tÄ… stronÄ…, analizuje HTML.

WyciÄ…ga wszystkie linki za pomocÄ… BeautifulSoup lub requests-html.

Filtruje tylko linki zawierajÄ…ce root URL.

Odrzuca linki zawierajÄ…ce # (niewnoszÄ…ce wartoÅ›ci).

Sprawdza, ktÃ³re linki juÅ¼ istniejÄ… w DynamoDB.

Te, ktÃ³re sÄ… nowe, zostajÄ…:

Zapisane do DynamoDB jako odwiedzone

Dodane do kolejki SQS (do dalszego crawlowania)

ğŸ” Kolejne iteracje:
KaÅ¼dy z nowo dodanych linkÃ³w trafia ponownie do funkcji crawler.

Mechanizm rekurencyjny trwa do momentu, aÅ¼:

Wszystkie linki zostanÄ… odwiedzone

W kolejce SQS nie bÄ™dzie juÅ¼ nowych wiadomoÅ›ci

ğŸš€ Deployment (SAM CLI)
sam build
sam deploy --guided
